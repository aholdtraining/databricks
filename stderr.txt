22/08/03 13:03:04 <console>:5: error: illegal character '\u201c'
       request_3.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/FileStore/df/request_3.csv”)
                                          ^
<console>:5: error: illegal character '\u201d'
       request_3.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/FileStore/df/request_3.csv”)
                                                                   ^
<console>:5: error: illegal character '\u201c'
       request_3.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/FileStore/df/request_3.csv”)
                                                                             ^
<console>:5: error: illegal character '\u201d'
       request_3.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/FileStore/df/request_3.csv”)
                                                                                    ^
<console>:5: error: illegal character '\u201c'
       request_3.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/FileStore/df/request_3.csv”)
                                                                                       ^
<console>:5: error: illegal character '\u201d'
       request_3.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/FileStore/df/request_3.csv”)
                                                                                            ^
<console>:5: error: illegal character '\u201c'
       request_3.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/FileStore/df/request_3.csv”)
                                                                                                    ^
<console>:5: error: illegal character '\u201d'
       request_3.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/FileStore/df/request_3.csv”)
                                                                                                                                               ^

/databricks/python/lib/python3.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.
  "You should import from traitlets.config instead.", ShimWarning)
/databricks/python/lib/python3.7/site-packages/IPython/nbconvert.py:13: ShimWarning: The `IPython.nbconvert` package has been deprecated since IPython 4.0. You should import from nbconvert instead.
  "You should import from nbconvert instead.", ShimWarning)
Wed Aug  3 13:12:03 2022 py4j imported
Wed Aug  3 13:12:03 2022 Python shell started with PID  7217  and guid  6427d052827846d9b033beecb72178d3
Wed Aug  3 13:12:03 2022 Initialized gateway on port 32981
Wed Aug  3 13:12:04 2022 Python shell executor start
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setLocalProperty' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setJobGroup' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
Dropped logging in PythonShell:

b'/local_disk0/tmp/1659496337193-0/PythonShell.py:806: DeprecationWarning: The `use_readline` parameter is deprecated and ignored since IPython 6.0.\n  parent=self,\n'
22/08/03 13:12:16 ERROR Uncaught throwable from user code: org.apache.spark.sql.AnalysisException: path dbfs:/FileStore/df/request_3.csv already exists.;
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:122)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:112)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:110)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:200)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:252)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:149)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:980)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:115)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:247)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:828)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:76)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:197)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:980)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:418)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:402)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:275)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:970)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-205345782029452:1)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-205345782029452:48)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-205345782029452:50)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-205345782029452:52)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$$iw$$iw$$iw$$iw.<init>(command-205345782029452:54)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$$iw$$iw$$iw.<init>(command-205345782029452:56)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$$iw$$iw.<init>(command-205345782029452:58)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$$iw.<init>(command-205345782029452:60)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read.<init>(command-205345782029452:62)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$.<init>(command-205345782029452:66)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$read$.<clinit>(command-205345782029452)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$eval$.$print$lzycompute(<notebook>:7)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$eval$.$print(<notebook>:6)
	at line9e11e8e5ed594061b25f0c4fbc5c287861.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:202)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:396)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
	at java.lang.Thread.run(Thread.java:748)
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setLocalProperty' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setJobGroup' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
---------------------------------------------------------------------------
AnalysisException                         Traceback (most recent call last)
<command-1179084142622429> in <module>
     13 sqlContext = SQLContext(sc)
     14 spark = SparkSession.builder.appName('Basics').getOrCreate()
---> 15 df_edw_dev = sqlContext.read.format("parquet").option("header", "false").option("delimiter", "|").option("sep", "|").load("dbfs:/mnt/rs08ue2qipadl03/FIONA/CDM/RBS/Item_Cost/Avg_Dly_Cost/businessdate=2022-08-01/daily_cost.parquet")
     16 df_1=df_edw_dev
     17 df_edw_prod = sqlContext.read.format("parquet").option("header", "false").option("delimiter", "|").option("sep", "|").load("dbfs:/mnt/rs05ue2pipadl03/FIONA/CDM/RBS/Item_Cost/Avg_Dly_Cost/businessdate=2022-08-01/daily_cost.parquet")

/databricks/spark/python/pyspark/sql/readwriter.py in load(self, path, format, schema, **options)
    176         self.options(**options)
    177         if isinstance(path, basestring):
--> 178             return self._df(self._jreader.load(path))
    179         elif path is not None:
    180             if type(path) != list:

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-> 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    132                 # Hide where the exception came from that shows a non-Pythonic
    133                 # JVM exception message.
--> 134                 raise_from(converted)
    135             else:
    136                 raise

/databricks/spark/python/pyspark/sql/utils.py in raise_from(e)

AnalysisException: Path does not exist: dbfs:/mnt/rs08ue2qipadl03/FIONA/CDM/RBS/Item_Cost/Avg_Dly_Cost/businessdate=2022-08-01/daily_cost.parquet;/databricks/python/lib/python3.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.
  "You should import from traitlets.config instead.", ShimWarning)
/databricks/python/lib/python3.7/site-packages/IPython/nbconvert.py:13: ShimWarning: The `IPython.nbconvert` package has been deprecated since IPython 4.0. You should import from nbconvert instead.
  "You should import from nbconvert instead.", ShimWarning)
Wed Aug  3 13:13:04 2022 py4j imported
Wed Aug  3 13:13:05 2022 Python shell started with PID  7307  and guid  209884be87464045b0456868416f1e03
Wed Aug  3 13:13:05 2022 Initialized gateway on port 34203
Wed Aug  3 13:13:05 2022 Python shell executor start
/databricks/python/lib/python3.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.
  "You should import from traitlets.config instead.", ShimWarning)
/databricks/python/lib/python3.7/site-packages/IPython/nbconvert.py:13: ShimWarning: The `IPython.nbconvert` package has been deprecated since IPython 4.0. You should import from nbconvert instead.
  "You should import from nbconvert instead.", ShimWarning)
Wed Aug  3 13:13:10 2022 py4j imported
Wed Aug  3 13:13:10 2022 Python shell started with PID  7656  and guid  72b18bcfa5fe469dae03e878783f9f26
Wed Aug  3 13:13:10 2022 Initialized gateway on port 43499
Wed Aug  3 13:13:11 2022 Python shell executor start
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setLocalProperty' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setJobGroup' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
Dropped logging in PythonShell:

b'/local_disk0/tmp/1659496337193-0/PythonShell.py:806: DeprecationWarning: The `use_readline` parameter is deprecated and ignored since IPython 6.0.\n  parent=self,\n'
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setLocalProperty' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setJobGroup' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
22/08/03 13:15:33 ERROR Uncaught throwable from user code: java.io.FileNotFoundException: File/5798555597390975/FileStore/jars/7396153e_a6eb_4f5d_a174_dbc87e293dcd does not exist.
	at shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.listStatus(NativeAzureFileSystem.java:2483)
	at com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:95)
	at com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:108)
	at com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:92)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:430)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:453)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:453)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:411)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:337)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:453)
	at com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:92)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:150)
	at com.databricks.backend.daemon.dbutils.FSUtils$.$anonfun$ls$1(DBUtilsCore.scala:86)
	at com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:81)
	at com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:85)
	at com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:34)
	at line07c4a1443ab34d81935b964d66d07e9b27.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-205345782029460:1)
	at line07c4a1443ab34d81935b964d66d07e9b27.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-205345782029460:43)
	at line07c4a1443ab34d81935b964d66d07e9b27.$read$$iw$$iw$$iw$$iw.<init>(command-205345782029460:45)
	at line07c4a1443ab34d81935b964d66d07e9b27.$read$$iw$$iw$$iw.<init>(command-205345782029460:47)
	at line07c4a1443ab34d81935b964d66d07e9b27.$read$$iw$$iw.<init>(command-205345782029460:49)
	at line07c4a1443ab34d81935b964d66d07e9b27.$read$$iw.<init>(command-205345782029460:51)
	at line07c4a1443ab34d81935b964d66d07e9b27.$read.<init>(command-205345782029460:53)
	at line07c4a1443ab34d81935b964d66d07e9b27.$read$.<init>(command-205345782029460:57)
	at line07c4a1443ab34d81935b964d66d07e9b27.$read$.<clinit>(command-205345782029460)
	at line07c4a1443ab34d81935b964d66d07e9b27.$eval$.$print$lzycompute(<notebook>:7)
	at line07c4a1443ab34d81935b964d66d07e9b27.$eval$.$print(<notebook>:6)
	at line07c4a1443ab34d81935b964d66d07e9b27.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:202)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:396)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
	at java.lang.Thread.run(Thread.java:748)
/databricks/python/lib/python3.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.
  "You should import from traitlets.config instead.", ShimWarning)
/databricks/python/lib/python3.7/site-packages/IPython/nbconvert.py:13: ShimWarning: The `IPython.nbconvert` package has been deprecated since IPython 4.0. You should import from nbconvert instead.
  "You should import from nbconvert instead.", ShimWarning)
Wed Aug  3 13:17:55 2022 py4j imported
Wed Aug  3 13:17:55 2022 Python shell started with PID  7924  and guid  23aaa737648b49b2878b74e3ff8fe500
Wed Aug  3 13:17:55 2022 Initialized gateway on port 35809
Wed Aug  3 13:17:55 2022 Python shell executor start
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setLocalProperty' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
/databricks/spark/python/pyspark/util.py:141: UserWarning: Currently, 'setJobGroup' (set to local properties) with multiple threads does not properly work. 
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. 
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. 
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
Dropped logging in PythonShell:

b'/local_disk0/tmp/1659496337193-0/PythonShell.py:806: DeprecationWarning: The `use_readline` parameter is deprecated and ignored since IPython 6.0.\n  parent=self,\n'
22/08/03 13:22:41 command-205345782029452:1: error: value txt is not a member of org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row]
final_item.repartition(1).write.txt("/mnt/rs06ue2dipadl03/FIONA/request_3.txt")
                                ^

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/databricks/spark/python/pyspark/sql/types.py in __getattr__(self, item)
   1594             # but this will not be used in normal cases
-> 1595             idx = self.__fields__.index(item)
   1596             return self[idx]

ValueError: 'Year' is not in list

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
<command-205345782029444> in <module>
      6   ##OutputPath= MOUNT_PATH_SPARKS + OutputBasePath
      7 
----> 8   FileDate = row.Year+'-'+row.Month+'-'+row.Day
      9   #dateFileExc = row.Year+row.Month+row.Day
     10   ##InputPath=InputPath+'/'+dateFldr+'/'+FileName+'_'+dateFileExc+'.'+RDSExtension

/databricks/spark/python/pyspark/sql/types.py in __getattr__(self, item)
   1598             raise AttributeError(item)
   1599         except ValueError:
-> 1600             raise AttributeError(item)
   1601 
   1602     def __setattr__(self, key, value):

AttributeError: Year---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<command-205345782029519> in <module>
      1 ConfigPath='dbfs:/mnt/rs06ue2dipadl03/FIONA/RDS/Ahold/Planogram/Facts/Shelf_Position/stsh_shelf_pstn_sdm_dates.cfg'
----> 2 df = spark.read.format('csv').option('header','true').option('SEP','|').load(MOUNT_PATH_SPARKS+ConfigPath)
      3 for row in df.rdd.collect():
      4   ## input parameters
      5 

NameError: name 'MOUNT_PATH_SPARKS' is not defined---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<command-205345782029519> in <module>
      1 ConfigPath='dbfs:/mnt/rs06ue2dipadl03/FIONA/RDS/Ahold/Planogram/Facts/Shelf_Position/stsh_shelf_pstn_sdm_dates.cfg'
----> 2 df = spark.read.format('csv').option('header','true').option('SEP','|').load(MOUNT_PATH_SPARKS+ConfigPath)
      3 for row in df.rdd.collect():
      4   ## input parameters
      5 

NameError: name 'MOUNT_PATH_SPARKS' is not defined---------------------------------------------------------------------------
AnalysisException                         Traceback (most recent call last)
<command-622833090183147> in <module>
     15   print(start)
     16   curr_file = s3_input_path+str(start.date().strftime("%Y"))+"/"+str(start.date().strftime("%m"))+"/"+str(start.date().strftime("%d"))+"/HOSTITEM__"+str(start.date().strftime("%Y%m%d"))+".txt"
---> 17   spark.read.csv("dbfs:/mnt/rs05ue2pipadl03/"+curr_file)
     18 
     19 

/databricks/spark/python/pyspark/sql/readwriter.py in csv(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)
    533             path = [path]
    534         if type(path) == list:
--> 535             return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
    536         elif isinstance(path, RDD):
    537             def func(iterator):

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-> 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    132                 # Hide where the exception came from that shows a non-Pythonic
    133                 # JVM exception message.
--> 134                 raise_from([0mconverted)
    135             else:
    136                 raise

/databricks/spark/python/pyspark/sql/utils.py in raise_from(e)

AnalysisException: Path does not exist: dbfs:/mnt/rs05ue2pipadl03/FIONA/RDS/Ahold/RELEX/MF/2021/01/02/HOSTITEM__20210102.txt;---------------------------------------------------------------------------
AnalysisException                         Traceback (most recent call last)
<command-622833090183147> in <module>
     15   print(start)
     16   curr_file = s3_input_path+str(start.date().strftime("%Y"))+"/"+str(start.date().strftime("%m"))+"/"+str(start.date().strftime("%d"))+"/HOSTITEM__"+str(start.date().strftime("%Y%m%d"))+".txt"
---> 17   spark.read.csv("dbfs:/mnt/rs05ue2pipadl03/"+curr_file)
     18 
     19 

/databricks/spark/python/pyspark/sql/readwriter.py in csv(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)
    533             path = [path]
    534         if type(path) == list:
--> 535             return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
    536         elif isinstance(path, RDD):
    537             def func(iterator):

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-> 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    132                 # Hide where the exception came from that shows a non-Pythonic
    133                 # JVM exception message.
--> 134                 raise_from([0mconverted)
    135             else:
    136                 raise

/databricks/spark/python/pyspark/sql/utils.py in raise_from(e)

AnalysisException: Path does not exist: dbfs:/mnt/rs05ue2pipadl03/FIONA/RDS/Ahold/RELEX/MF/2021/01/21/HOSTITEM__20210121.txt;